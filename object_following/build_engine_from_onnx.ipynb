{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "peripheral-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "three-allowance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx==1.6.0\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\",)': /packages/81/a9/a14c3bc32908c37b46b19a89eb6185b0c90fd9c03ef12379d51940b8fc71/onnx-1.6.0.tar.gz\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/81/a9/a14c3bc32908c37b46b19a89eb6185b0c90fd9c03ef12379d51940b8fc71/onnx-1.6.0.tar.gz (3.1MB)\n",
      "\u001b[K    100% |################################| 3.1MB 145kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx==1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx==1.6.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx==1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx==1.6.0)\n",
      "Building wheels for collected packages: onnx\n",
      "  Running setup.py bdist_wheel for onnx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/38/61/aa76b47ce878c92b3d6944fe75e16eef5a96944f53462a519d\n",
      "Successfully built onnx\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.6.0\n",
      "Collecting tf2onnx==1.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/77/82/80cc3ddb4d1adb8250f83b5f27d9fb203c1549a3acf26d7ebc9309a20931/tf2onnx-1.7.1-py3-none-any.whl (194kB)\n",
      "\u001b[K    100% |################################| 204kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.1)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tf2onnx==1.7.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx>=1.4.1->tf2onnx==1.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.4.1->tf2onnx==1.7.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tf2onnx==1.7.1)\n",
      "Installing collected packages: tf2onnx\n",
      "Successfully installed tf2onnx-1.7.1\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install onnx==1.6.0\n",
    "# !pip3 install tf2onnx==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adverse-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import onnx\n",
    "import tf2onnx\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-termination",
   "metadata": {},
   "source": [
    "### Converting Tensorflow model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "static-intelligence",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GraphDef' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-93697c4604e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0moutput_onnx_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ssd_mobilenet.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mconvert_frozen_graph_to_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozen_graph_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_onnx_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-93697c4604e7>\u001b[0m in \u001b[0;36mconvert_frozen_graph_to_onnx\u001b[0;34m(frozen_graph_path, output_onnx_path, opset)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m# Import the graph and convert variables to constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Simplified Session creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mfrozen_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_variables_to_constants_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0minput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrozen_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrozen_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36mconvert_variables_to_constants_v2\u001b[0;34m(func, lower_control_flow)\u001b[0m\n\u001b[1;32m    400\u001b[0m   \u001b[0;31m# TODO(nupurgarg): Replace ResourceGather with Gather.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m   \u001b[0;31m# Inline the graph in order to remove functions when possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m   \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_inline_graph_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_control_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[0;31m# Gets list of all node defs include those in the library.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36m_run_inline_graph_optimization\u001b[0;34m(func, lower_control_flow)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mGraphDef\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m   \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlower_control_flow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_lower_using_switch_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GraphDef' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import Session # Only import Session from compat.v1\n",
    "from tensorflow.compat.v1 import GraphDef\n",
    "\n",
    "def convert_frozen_graph_to_onnx(frozen_graph_path, output_onnx_path, opset=11):\n",
    "  \"\"\"\n",
    "  Converts a frozen TensorFlow graph to ONNX format.\n",
    "\n",
    "  Args:\n",
    "    frozen_graph_path: Path to the frozen graph (.pb) file.\n",
    "    output_onnx_path: Path to save the ONNX model (.onnx) file.\n",
    "    opset: The ONNX opset version to target (default: 11).\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the frozen graph\n",
    "#   with tf.io.gfile.GFile(frozen_graph_path, \"rb\") as f:\n",
    "#     graph_def = tf.compat.v1.GraphDef()\n",
    "#     graph_def.ParseFromString(f.read())\n",
    "\n",
    "  with tf.gfile.GFile(\"SSD-Mobilenet-V2-tf/frozen_inference_graph.pb\", \"rb\") as f:\n",
    "    graph_def = GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    \n",
    "  # Import the graph and convert variables to constants\n",
    "  with tf.Session() as sess:  # Simplified Session creation\n",
    "    frozen_func = convert_variables_to_constants_v2(sess.graph.as_graph_def())\n",
    "    input_names = [inp.name for inp in frozen_func.inputs]\n",
    "    output_names = [out.name for out in frozen_func.outputs]\n",
    "    print(\"Inputs: \", input_names)\n",
    "    print(\"Outputs: \", output_names)\n",
    "\n",
    "    # Convert to ONNX\n",
    "    import tf2onnx  # Assuming tf2onnx is installed\n",
    "    onnx_model = tf2onnx.tfonnx.process_tf_graph(\n",
    "        graph=frozen_func,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        opset=opset\n",
    "    )\n",
    "    model_proto = onnx_model.make_model(\"converted_model\")\n",
    "    with open(output_onnx_path, \"wb\") as f:\n",
    "      f.write(model_proto.SerializeToString())\n",
    "\n",
    "# Path to your frozen graph\n",
    "frozen_graph_path = \"SSD-Mobilenet-V2-tf/frozen_inference_graph.pb\"\n",
    "# Path to save the ONNX model\n",
    "output_onnx_path = \"ssd_mobilenet.onnx\"\n",
    "\n",
    "convert_frozen_graph_to_onnx(frozen_graph_path, output_onnx_path)\n",
    "\n",
    "\n",
    "# # Function to convert frozen graph to ONNX\n",
    "# def convert_frozen_graph_to_onnx(frozen_graph_path, output_onnx_path):\n",
    "#     # Load the frozen graph\n",
    "#     with tf.io.gfile.GFile(frozen_graph_path, \"rb\") as f:\n",
    "#         graph_def = tf.compat.v1.GraphDef()\n",
    "#         graph_def.ParseFromString(f.read())\n",
    "\n",
    "#     # Import the graph into a new graph and convert variables to constants\n",
    "#     with tf.Graph().as_default() as graph:\n",
    "#         tf.import_graph_def(graph_def, name=\"\")\n",
    "#         with tf.compat.v1.Session(graph=graph) as sess:\n",
    "#             frozen_func = convert_variables_to_constants_v2(sess.graph.as_graph_def())\n",
    "#             input_names = [inp.name for inp in frozen_func.inputs]\n",
    "#             output_names = [out.name for out in frozen_func.outputs]\n",
    "#             print(\"Inputs: \", input_names)\n",
    "#             print(\"Outputs: \", output_names)\n",
    "\n",
    "#             # Convert to ONNX\n",
    "#             onnx_model = tf2onnx.tfonnx.process_tf_graph(\n",
    "#                 graph_def=frozen_func,\n",
    "#                 input_names=input_names,\n",
    "#                 output_names=output_names,\n",
    "#                 opset=11\n",
    "#             )\n",
    "#             model_proto = onnx_model.make_model(\"converted_model\")\n",
    "#             with open(output_onnx_path, \"wb\") as f:\n",
    "#                 f.write(model_proto.SerializeToString())\n",
    "\n",
    "# # Path to your frozen graph\n",
    "# frozen_graph_path = \"SSD-Mobilenet-V2-tf/frozen_inference_graph.pb\"\n",
    "# # Path to save the ONNX model\n",
    "# output_onnx_path = \"ssd_mobilenet.onnx\"\n",
    "\n",
    "# convert_frozen_graph_to_onnx(frozen_graph_path, output_onnx_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-template",
   "metadata": {},
   "source": [
    "### Converting ONNX to TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "trtexec --onnx=ssd_mobilenet.onnx --saveEngine=ssd_mobilenet.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-ontario",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
